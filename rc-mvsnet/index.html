
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

	<link rel="shortcut icon" href="./img/doge.ico">
    <title>RC-MVSNet</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:image" content="https://jonbarron.info/mipnerf/img/rays_square.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://mizhenxing.github.io/gbinet"/>
    <meta property="og:title" content="mip-NeRF" />
    <meta property="og:description" content="Project page for Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields." />

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="mip-NeRF" />
    <meta name="twitter:description" content="Project page for Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields." />
    <meta name="twitter:image" content="https://jonbarron.info/mipnerf/img/rays_square.png" />


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <!-- <b>GBi-Net</b>: A Multiscale Representation <br> for Anti-Aliasing Neural Radiance Fields</br>  -->
                RC-MVSNet: Unsupervised Multi-View Stereo <br> with Neural Rendering </br>
                <small>
                    ECCV 2022
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://boese0601.github.io/">
                            Di Chang</a><sup>1</sup>
                    </li>
                    <li>
                        <a href="https://aljazbozic.github.io/">
                            Aljaž Božič</a><sup>1</sup>
                    </li>
                    <li>
                        <a href="https://people.epfl.ch/tong.zhang?lang=en">
                            Tong Zhang</a><sup>2</sup>
                    </li>
                    <li>
                            Qingsong Yan<sup>3</sup>
                    </li>
                    <li>
                            Yingcong Chen<sup>3</sup>
                    </li>
                    <br>
                    <li>
                        <a href="https://people.epfl.ch/sabine.susstrunk">
                            Sabine Süsstrunk</a><sup>2</sup>
                    </li>
                    <li>
                        <a href="https://niessnerlab.org/">
                            Matthias Nießner</a><sup>1</sup>
                    </li>
                    <br>
                    <li>
                        <sup>1</sup><a href="https://www.tum.de/">Technische Universität München</a>
                        <br>
                        <sup>2</sup><a href="https://www.epfl.ch/">École Polytechnique Fédérale de Lausanne</a>
                        <br>
                        <sup>3</sup><a href="https://hkust.edu.hk/">The Hong Kong University of Science and Technology</a>
                    </li>
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-6 col-md-offset-3 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2203.03949">
                            <image src="img/arxiv-logo.png" height="60px">
                                <h4><strong>Arxiv</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="">
                            <image src="img/youtube_icon.png" height="60px">
                                <h4><strong>Video(Coming Soon)</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/Boese0601/RC-MVSNet">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <image src="img/Comparison.jpg" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
                    Finding accurate correspondences among different views is
                    the Achilles' heel of unsupervised Multi-View Stereo (MVS). Existing
                    methods are built upon the assumption that corresponding pixels share
                    similar photometric features. However, multi-view images in real scenarios observe non-Lambertian surfaces and experience occlusions. In this
                    work, we propose a novel approach with neural rendering (RC-MVSNet)
                    to solve such ambiguity issues of correspondences among views. Specifically, we impose a depth rendering consistency loss to constrain the
                    geometry features close to the object surface to alleviate occlusions.
                    Concurrently, we introduce a reference view synthesis loss to generate
                    consistent supervision, even for non-Lambertian surfaces. Extensive experiments on DTU and Tanks&Temples benchmarks demonstrate that
                    our approach achieves state-of-the-art performance over unsupervised MVS frameworks and competitive performance to many supervised methods. Our code will be released at <a href="https://github.com/Boese0601/RC-MVSNet">RC-MVSNet</a>.
                </p>
            </div>
        </div>


        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://www.youtube.com/embed/EpH175PY1A0" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div> -->


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Proposed Method
                </h3>
                <image src="img/Pipe.jpg" class="img-responsive" alt="overview"><br>
                <!-- <p class="text-justify">
                    Typical positional encoding (as used in Transformer networks and Neural Radiance Fields) maps a single point in space to a feature vector, where each element is generated by a sinusoid with an exponentially increasing frequency:
                </p> -->
                <p class="text-justify">
                    Overview of our RC-MVSNet. a) Unsupervised Backbone CasMVSNet predicts initial depth map by photometric consistency and provides depth priors for rendering consistency network. b) Rendering Consistency Network generates image and depth by neural rendering under the guidance of depth priors. c) The rendered image is supervised by the reference view synthesis loss. d) The rendered depth is supervised by the depth rendering consistency loss.
                </p>
            </div>
        </div>
            


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Motivation
                </h3>
                <image src="img/Motivation.jpg" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
                    In the real-world environment occlusions, reflecting, non-Lambertian surfaces, varying camera exposure, and other variables will make photometric consistency assumption invalid.
                </p>

            </div>
        </div>
            


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results on DTU
                </h3>
                <h4>
                    Point Cloud Reconstruction
                </h4>
                <image src="img/dtu_all.jpg" class="img-responsive" alt="overview"><br>
                <!-- <p class="text-justify">
                    We train NeRF and mip-NeRF on a dataset with images at four different resolutions. Normal NeRF (left) is not capable of learning to represent the same scene at multiple levels of detail, with blurring in close-up shots and aliasing in low resolution views, while mip-NeRF (right) both preserves sharp details in close-ups and correctly renders the zoomed-out images.
                </p> -->
                <h4>
                    Depth map Inference
                </h4>
                <image src="img/Depthmaps.jpg" class="img-responsive" alt="overview"><br>

            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results on Tanksandtemples
                </h3>
                <image src="img/tanks.jpg" class="img-responsive" alt="overview"><br>
                <image src="img/tanksandtemples.jpg" class="img-responsive" alt="overview"><br>
                <!-- <p class="text-justify">
                    We train NeRF and mip-NeRF on a dataset with images at four different resolutions. Normal NeRF (left) is not capable of learning to represent the same scene at multiple levels of detail, with blurring in close-up shots and aliasing in low resolution views, while mip-NeRF (right) both preserves sharp details in close-ups and correctly renders the zoomed-out images.
                </p> -->
            </div>
        </div>
      
            
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@inproceedings{chang2022rc,
title={RC-MVSNet: Unsupervised Multi-View Stereo with Neural Rendering},
author={Chang, Di and Bo{\v{z}}i{\v{c}}, Alja{\v{z}} and Zhang, Tong and Yan, Qingsong and Chen, Yingcong and S{\"u}sstrunk, Sabine and Nie{\ss}ner, Matthias},
booktitle={Proceedings of the European conference on computer vision (ECCV)},
year={2022}
}</textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                    This work is completed during Di's Guided Research Praktikum at Visual Computing & Artificial Intelligence Group(TUM) directed by Prof. Matthias Nießner and Summer@EPFL Internship at Image and Visual Representation Lab(EPFL) directed by Prof. Sabine Süsstrunk. The project is funded by the ERC Starting Grant Scan2CAD (804724), a TUM-IAS Rudolf Mößbauer Fellowship, and the German Research Foundation (DFG) Grant Making Machine Learning on Static and Dynamic 3D Data Practical. 
                    <br>
                The website template was borrowed from <a href="https://mizhenxing.github.io/">Zhenxing Mi</a> <a href="https://mizhenxing.github.io/gbinet/">GBi-Net</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
