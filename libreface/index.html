
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

	<link rel="shortcut icon" href="./img/doge.ico">
    <title>LibreFace</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:image" content="https://boese0601.github.io/images/libre.jpg">
    <meta property="og:image:type" content="image/jpg">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://boese0601.github.io/libreface/"/>
    <meta property="og:title" content="LibreFace" />
    <meta property="og:description" content="Project page for LibreFace: An Open-Source Toolkit for Deep Facial Expression Analysis." />

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="LibreFace" />
    <meta name="twitter:description" content="Project page for LibreFace: An Open-Source Toolkit for Deep Facial Expression Analysis." />
    <meta name="twitter:image" content="https://boese0601.github.io/images/libre.jpg" />


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
    <style>
        .wrap{
          height: 210px;
          overflow-x: auto;
          overflow-y: hidden;
          white-space: nowrap;
        }
        .b{
          width: 250px;
          height: 200px;
        }
        </style>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <!-- <b>GBi-Net</b>: A Multiscale Representation <br> for Anti-Aliasing Neural Radiance Fields</br>  -->
                LibreFace: An Open-Source Toolkit for Deep Facial Expression Analysis </br>
                <small>
                    WACV 2024
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://boese0601.github.io/">
                            Di Chang</a>
                    </li>
                    <li>
                        <a href="https://yufengyin.github.io/">
                            Yufeng Yin</a>
                    </li>
                    <li>
                        Zongjian Li
                    </li>
                    <li>
                        <a href="https://scholar.google.com/citations?user=HuuQRj4AAAAJ&hl=en">
                            Minh Tran
                    </li>

                    <li>
                        <a href="https://people.ict.usc.edu/~soleymani/">
                            Mohammad Soleymani</a>
                    </li>
                    <br>
                    <li>
                        <a href="https://ict.usc.edu/">Institute for Creative Technologies, University of Southern California</a>
                    </li>
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-6 col-md-offset-3 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="">
                            <image src="img/arxiv-logo.png" height="60px">
                                <h4><strong>Arxiv</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="">
                            <image src="img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/ihp-lab/LibreFace">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <image src="img/Pipe.png" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
                Facial expression analysis is an important tool for human-computer interaction. 
                In this paper, we introduce LibreFace, an open-source toolkit for facial expression analysis. 
                This open-source toolbox offers real-time and offline analysis of facial behavior through deep learning models, including facial action unit (AU) detection, AU intensity estimation, and facial expression recognition. 
                To accomplish this, we employ several techniques, including the utilization of a large-scale pre-trained network, feature-wise knowledge distillation, and task-specific fine-tuning. 
                These approaches are designed to effectively and accurately analyze facial expressions by leveraging visual information, thereby facilitating the implementation of real-time interactive applications. 
                In terms of Action Unit (AU) intensity estimation, we achieve a Pearson Correlation Coefficient (PCC) of <b>0.63</b> on DISFA, 
                which is <b>7%</b> higher than the performance of OpenFace 2.0, while maintaining highly-efficient inference that 
                runs <b>two times</b> faster than OpenFace 2.0. 
                Despite being compact, our model also demonstrates competitive performance to state-of-the-art facial expression analysis methods on AffecNet, FFHQ, and RAF-DB.
                Our code will be released soon at <a href="https://github.com/ihp-lab/LibreFace">LibreFace</a>.
                </p>
            </div>
        </div>


        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://www.youtube.com/embed/EpH175PY1A0" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div> -->
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://www.youtube.com/embed/I_Q47TxTLbs" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>

        <!-- <iframe width="1191" height="670" src="" title="RC-MVSNet: Unsupervised Multi-View Stereo with Neural Rendering (ECCV'22)" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Proposed Method
                </h3>
                <div class="col-md-offset-3">
                    <img src="img/method.jpg" class="img-responsive" alt="overview" width="450px"><br>
                </div>
         
                
                <!-- <p class="text-justify">
                    Typical positional encoding (as used in Transformer networks and Neural Radiance Fields) maps a single point in space to a feature vector, where each element is generated by a sinusoid with an exponentially increasing frequency:
                </p> -->
                <p class="text-justify">
                    We first pre-process the input images or video, which involves face mesh and landmark detection and image alignment.
                    Next, we feed the pre-processed images into a pre-trained MAE encoder, followed by a linear regression or classification layer that predicts the AU intensity values or facial expression labels. 
                    Once the MAE is fine-tuned, we employ feature-wise distillation to transfer the teacher model's (MAE) knowledge to a lightweight student model (ResNet-18). 
                    Finally, we use the distilled ResNet-18 for efficient Action Unit Intensity Estimation, Action Unit Detection and Facial Expression Recognition.
                    All outputs from the pipeline can be saved, and training/inference code is open-sourced.
                </p>
            </div>
        </div>
            


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Contributions
                </h3>
                <div class="col-md-offset-4">
                    <image src="img/System.png" class="img-responsive" alt="overview" width="300px"><br>
                </div>
                
                <p class="text-justify">
                We present LibreFace, an open-source and comprehensive toolkit for accurate and real-time facial expression analysis with both CPU-only and GPU-acceleration versions. LibreFace eliminates the gap between cutting-edge research and an easy and free-to-use non-commercial toolbox.
                We propose to adaptively pre-train the vision encoders with various face datasets and then distillate them to a lightweight ResNet-18 model in a feature-wise matching manner.
                We conduct extensive experiments of pre-training and distillation to demonstrate that our proposed pipeline achieves comparable results to state-of-the-art works while maintaining real-time efficiency.
                LibreFace system supports cross-platform running, and the code is open-sourced in C\# (model inference and checkpoints) and Python (model training, inference, and checkpoints).

                </p>

            </div>
        </div>
            


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Software Demo
                </h3>
                <image src="img/software.png" class="img-responsive" alt="overview"><br>
                <!-- <p class="text-justify">
                    We train NeRF and mip-NeRF on a dataset with images at four different resolutions. Normal NeRF (left) is not capable of learning to represent the same scene at multiple levels of detail, with blurring in close-up shots and aliasing in low resolution views, while mip-NeRF (right) both preserves sharp details in close-ups and correctly renders the zoomed-out images.
                </p> -->
                
                <!-- <section class="hero is-light is-small">
                    <div class="wrap">
                        <video poster="" id="sample1_input" autoplay controls muted loop height="100%" class="b">
                            <source src="video/ours001.mp4"
                                    type="video/mp4">
                        </video>
                        <video poster="" id="sample1_input" autoplay controls muted loop height="100%" class="b">
                            <source src="video/ours033.mp4"
                                    type="video/mp4">
                        </video>
                        <video poster="" id="sample1_input" autoplay controls muted loop height="100%" class="b">
                            <source src="video/ours114.mp4"
                                    type="video/mp4">
                        </video>
                        <video poster="" id="sample1_input" autoplay controls muted loop height="100%" class="b">
                            <source src="video/ours009.mp4"
                                    type="video/mp4">
                        </video>
                      </div>
                </section> -->
                <!-- <h4>
                    Depth map Inference
                </h4>
                <image src="img/Depthmaps.jpg" class="img-responsive" alt="overview"><br> -->

            </div>
        </div>


        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results on Tanksandtemples
                </h3>
                <image src="img/tanks.jpg" class="img-responsive" alt="overview"><br>
                <image src="img/tanksandtemples.jpg" class="img-responsive" alt="overview"><br>
            </div>
        </div>
       -->
            
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
                    @inproceedings{chang2022rc,
                    title={RC-MVSNet: Unsupervised Multi-View Stereo with Neural Rendering},
                    author={Chang, Di and Bo{\v{z}}i{\v{c}}, Alja{\v{z}} and Zhang, Tong and Yan, Qingsong and Chen, Yingcong and S{\"u}sstrunk, Sabine and Nie{\ss}ner, Matthias},
                    booktitle={Proceedings of the European conference on computer vision (ECCV)},
                    year={2022}
                    }</textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                    This work is sponsored by the U.S. Army Research Laboratory (ARL) under contract number W911NF-14-D-0005. The content of the information does not necessarily reflect the position or the policy of the Government, and no official endorsement should be inferred.
                    <br>
                </p>
            </div>
        </div>
    </div>
</body>
</html>
